{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('./data/')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force to the last GPU (Adam - for now)\n",
    "\n",
    "DEVICE = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k', device=DEVICE)\n",
    "tokenizer = open_clip.get_tokenizer('ViT-H-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_parquet('mscoco.parquet').TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0])\n",
    "\n",
    "# Test for one token to get dims and everything correct\n",
    "test_toks = tokenizer(text[0]).to(DEVICE)\n",
    "print(test_toks.shape)\n",
    "print(test_toks)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    features = model.encode_text(test_toks)\n",
    "    features /= features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(features.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "The White House announced on Saturday that Joe Biden was returning to Washington from out of town “to consult with his national security team about events in the Middle East” amid heightened tension between Israel and Iran.\n",
    "\n",
    "A military helicopter hovering over a cargo vessel\n",
    "World waits anxiously for Iranian response to Israel’s killing of top general\n",
    "Read more\n",
    "The US president had been due to spend the weekend in Delaware at his residence in Rehoboth Beach but early on Saturday afternoon set off at short notice to return to the White House.\n",
    "\n",
    "This followed Biden saying on Friday that he expects an Iranian attack on Israel “sooner rather than later” and issued a last-ditch message to Tehran, saying: “Don’t.”\n",
    "\n",
    "Earlier, John Kirby, the White House national security spokesperson, had warned that the threat of a significant Iranian attack on Israel remained “viable” despite Washington-led efforts, including calls to Tehran from the UK and Germany, to deter a serious escalation in the conflict in the Middle East.\n",
    "\n",
    "On Saturday, Iran’s paramilitary Revolutionary Guard Corps in the strait of Hormuz, 50 nautical miles off the coast of the United Arab Emirates, seized an Israeli-affiliated container ship.\n",
    "'''\n",
    "print(example)\n",
    "\n",
    "# Test for one token to get dims and everything correct\n",
    "test_toks = tokenizer(example).to(DEVICE)\n",
    "print(test_toks.shape)\n",
    "print(test_toks)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    features = model.encode_text(test_toks)\n",
    "    features /= features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(features.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore, our output features are n x 1024\n",
    "# Necessary for batching this output. We will allocate a tensor of that \n",
    "#   length and then continually place the features in the correct indxs\n",
    "#   according to batch_size\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "num_batches = math.ceil(text.shape[0] / BATCH_SIZE)\n",
    "\n",
    "# Preallocate output tensor\n",
    "out_toks = torch.zeros(text.shape[0], 77)\n",
    "out_feats = torch.zeros(text.shape[0], 1024)\n",
    "\n",
    "for bn in tqdm(range(num_batches)):\n",
    "    # Get batch toks\n",
    "    tokens = tokenizer(text[BATCH_SIZE*bn:min(BATCH_SIZE*(bn+1), text.shape[0])]).to(DEVICE)\n",
    "\n",
    "    # Place tokens in output\n",
    "    out_toks[BATCH_SIZE*bn:min(BATCH_SIZE*(bn+1), text.shape[0]), :] = tokens\n",
    "\n",
    "    # Encode text\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        features = model.encode_text(tokens)\n",
    "        features /= features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Place them in output\n",
    "    out_feats[BATCH_SIZE*bn:min(BATCH_SIZE*(bn+1), text.shape[0]), :] = features\n",
    "\n",
    "torch.save(out_toks, output_dir / 'tokens.pt')\n",
    "torch.save(out_feats, output_dir / 'features.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-gen-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
