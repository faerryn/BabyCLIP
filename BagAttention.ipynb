{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('./data/')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:3\n"
     ]
    }
   ],
   "source": [
    "# Force to the last GPU (Adam - for now)\n",
    "\n",
    "DEVICE = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k', device=DEVICE)\n",
    "tokenizer = open_clip.get_tokenizer('ViT-H-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_parquet('mscoco.parquet').TEXT.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A man with a red helmet on a small moped on a dirt road. '\n",
      " 'Man riding a motor bike on a dirt road on the countryside.'\n",
      " 'A man riding on the back of a motorcycle.'\n",
      " 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. '\n",
      " 'A man in a red shirt and a red hat is on a motorcycle on a hill side.'\n",
      " 'A woman wearing a net on her head cutting a cake. '\n",
      " 'A woman cutting a large white sheet cake.'\n",
      " 'A woman wearing a hair net cutting a large sheet cake.'\n",
      " 'there is a woman that is cutting a white cake'\n",
      " \"A woman marking a cake with the back of a chef's knife. \"]\n"
     ]
    }
   ],
   "source": [
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77])\n",
      "tensor([[49406,   320,   786,   593,   320,   736, 11122,   525,   320,  2442,\n",
      "           617,  2966,   525,   320, 11795,  1759,   269, 49407,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], device='cuda:3')\n",
      "torch.Size([1, 1024])\n",
      "tensor([[ 0.0338,  0.0446, -0.0328,  ..., -0.0190,  0.0113, -0.0351]],\n",
      "       device='cuda:3', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Test for one token to get dims and everything correct\n",
    "test_toks = tokenizer(text[0]).to(DEVICE)\n",
    "print(test_toks.shape)\n",
    "print(test_toks)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    features = model.encode_text(test_toks)\n",
    "    features /= features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(features.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/578 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 578/578 [08:00<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Therefore, our output features are n x 1024\n",
    "# Necessary for batching this output. We will allocate a tensor of that \n",
    "#   length and then continually place the features in the correct indxs\n",
    "#   according to batch_size\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "num_batches = math.ceil(text.shape[0] / BATCH_SIZE)\n",
    "\n",
    "# Preallocate output tensor\n",
    "out_toks = torch.zeros(text.shape[0], 77)\n",
    "out_feats = torch.zeros(text.shape[0], 1024)\n",
    "\n",
    "for bn in tqdm(range(num_batches)):\n",
    "    # Get batch toks\n",
    "    tokens = tokenizer(text[BATCH_SIZE*bn:min(BATCH_SIZE*(bn+1), text.shape[0])]).to(DEVICE)\n",
    "\n",
    "    # Place tokens in output\n",
    "    out_toks[BATCH_SIZE*bn:min(BATCH_SIZE*(bn+1), text.shape[0]), :] = tokens\n",
    "\n",
    "    # Encode text\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        features = model.encode_text(tokens)\n",
    "        features /= features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Place them in output\n",
    "    out_feats[BATCH_SIZE*bn:min(BATCH_SIZE*(bn+1), text.shape[0]), :] = features\n",
    "\n",
    "torch.save(out_toks, output_dir / 'tokens.pt')\n",
    "torch.save(out_feats, output_dir / 'features.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-gen-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
