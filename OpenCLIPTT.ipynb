{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using devices [2, 3]\n"
     ]
    }
   ],
   "source": [
    "DEVICES = [2, 3]\n",
    "print(f'Using devices {DEVICES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep-gen/deep-gen-venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen_clip\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m og_model, _, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mopen_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mViT-H-14\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlaion2b_s32b_b79k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICES\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mget_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-H-14\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/deep-gen-venv/lib/python3.8/site-packages/open_clip/factory.py:384\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, **model_kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_and_transforms\u001b[39m(\n\u001b[1;32m    361\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    362\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    380\u001b[0m ):\n\u001b[1;32m    381\u001b[0m     force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[1;32m    382\u001b[0m         {}, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, interpolation\u001b[38;5;241m=\u001b[39mimage_interpolation, resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode)\n\u001b[0;32m--> 384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_hf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m     pp_cfg \u001b[38;5;241m=\u001b[39m PreprocessCfg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mpreprocess_cfg)\n\u001b[1;32m    404\u001b[0m     preprocess_train \u001b[38;5;241m=\u001b[39m image_transform_v2(\n\u001b[1;32m    405\u001b[0m         pp_cfg,\n\u001b[1;32m    406\u001b[0m         is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    407\u001b[0m         aug_cfg\u001b[38;5;241m=\u001b[39maug_cfg,\n\u001b[1;32m    408\u001b[0m     )\n",
      "File \u001b[0;32m~/deep-gen-venv/lib/python3.8/site-packages/open_clip/factory.py:276\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, **model_kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m pretrained_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n",
      "File \u001b[0;32m~/deep-gen-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep-gen-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/deep-gen-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/deep-gen-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/deep-gen-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep-gen-venv/lib/python3.8/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "\n",
    "og_model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k', device=DEVICES[0])\n",
    "tokenizer = open_clip.get_tokenizer('ViT-H-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "49408\n",
      "Embedding(49408, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(og_model.context_length)\n",
    "print(og_model.vocab_size)\n",
    "print(og_model.token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = og_model.context_length\n",
    "EMBED_DIM = 1024\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, conv_depth=3, ffn_depth=3, norm=True):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(SEQ_LEN, EMBED_DIM), requires_grad=True)\n",
    "        \n",
    "        conv_block = []\n",
    "        for i in range(conv_depth):\n",
    "            conv_block.append(nn.Conv1d(EMBED_DIM, EMBED_DIM, 3, padding=1))\n",
    "            if i + 1 == conv_depth:\n",
    "                conv_block.append(nn.ReLU())\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "        \n",
    "        pos_wise_ffn = []\n",
    "        for i in range(ffn_depth):\n",
    "            pos_wise_ffn.append(nn.Linear(EMBED_DIM, EMBED_DIM))\n",
    "            if i + 1 == ffn_depth:\n",
    "                pos_wise_ffn.append(nn.ReLU())\n",
    "        self.pos_wise_ffn = nn.Sequential(*pos_wise_ffn)\n",
    "\n",
    "        self.norm = nn.LayerNorm(EMBED_DIM) if norm else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len, batch_size, _ = x.shape\n",
    "\n",
    "        z = x + self.pos_encoding[:seq_len, None, :]\n",
    "\n",
    "        z = z.permute(1, 2, 0)\n",
    "        z = self.conv_block(z)\n",
    "        \n",
    "        z = z.flatten(0, 1)\n",
    "        z = self.pos_wise_ffn(z)\n",
    "        \n",
    "        if self.norm:\n",
    "            z = self.norm(z)\n",
    "\n",
    "        z = z.view(batch_size, seq_len, EMBED_DIM).permute(1, 0, 2)\n",
    "        return x + z\n",
    "\n",
    "class ConvIsAllYouNeed(nn.Module):\n",
    "    def __init__(self, conv_depth=3, ffn_depth=3, blocks=3, dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        seq = []\n",
    "        for i in range(blocks):\n",
    "            is_not_last = (i + 1 < blocks)\n",
    "            seq.append(EncoderBlock(conv_depth, ffn_depth, is_not_last))\n",
    "            if is_not_last and dropout > 0:\n",
    "                seq.append(nn.Dropout(dropout))\n",
    "\n",
    "\n",
    "    def get_cast_dtype(self):\n",
    "        return torch.float32\n",
    "\n",
    "    \"\"\"\n",
    "        LND -> LND s.t. [n_ctx, batch_size, d_model]\n",
    "    \"\"\"\n",
    "    def forward(self, x, attn_mask = None):\n",
    "        seq_len, batch_size, _ = x.shape\n",
    "\n",
    "        x = x / torch.norm(x, 2, dim=-1, keepdim=True)\n",
    "        return x\n",
    "\n",
    "class BadNet(nn.Module):\n",
    "    def __init__(self, latent_dim=32, depth=3): #, conv_depth=3, ffn_depth=3, blocks=3, dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.down = nn.Conv1d(EMBED_DIM, latent_dim, 1)\n",
    "        self.up = nn.Conv1d(latent_dim, EMBED_DIM, 1)\n",
    "        \n",
    "        self.ffn = nn.ModuleList([nn.Linear(latent_dim * SEQ_LEN, latent_dim * SEQ_LEN) for _ in range(depth)])\n",
    "\n",
    "    def get_cast_dtype(self):\n",
    "        return torch.float32\n",
    "\n",
    "    \"\"\"\n",
    "        LND -> LND s.t. [n_ctx, batch_size, d_model]\n",
    "    \"\"\"\n",
    "    def forward(self, x, attn_mask = None):\n",
    "        seq_len, batch_size, _ = x.shape\n",
    "\n",
    "        x = x.flatten(0, 1)[..., None]\n",
    "        x = self.down(x)\n",
    "        x = x.view(seq_len, batch_size, -1).permute(1, 0, 2).flatten(1, 2)\n",
    "        \n",
    "        for i, linear in enumerate(self.ffn):\n",
    "            x = x + linear(x)\n",
    "            if i + 1 < len(self.ffn):\n",
    "                x = nn.functional.relu(x)\n",
    "        \n",
    "        x = x.view(batch_size, seq_len, -1).flatten(0, 1)[..., None]\n",
    "        x = self.up(x)\n",
    "        x = x.view(batch_size, seq_len, EMBED_DIM).permute(1, 0, 2)\n",
    "        \n",
    "        # x = x / torch.norm(x, 2, dim=-1, keepdim=True)\n",
    "        return x\n",
    "\n",
    "class NothingIsAllYouNeed(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def get_cast_dtype(self):\n",
    "        return torch.float32\n",
    "\n",
    "    def forward(self, x, attn_mask = None):\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, halvings=2, conv_depth=2, latent_depth=3, conv_dropout=0.1, dense_dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(SEQ_LEN, EMBED_DIM), requires_grad=True)\n",
    "        latent_dim = EMBED_DIM // (2**halvings)\n",
    "        \n",
    "        self.down = nn.ModuleList([\n",
    "            nn.Sequential(*[nn.Sequential(\n",
    "                    nn.Conv1d(EMBED_DIM // (2**i), EMBED_DIM // (2**i), 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(conv_dropout),\n",
    "                    nn.BatchNorm1d(EMBED_DIM // (2**i)),\n",
    "                ) for _ in range(conv_depth)])\n",
    "            for i in range(halvings)])\n",
    "        \n",
    "        self.up   = nn.ModuleList([\n",
    "            nn.Sequential(*[nn.Sequential(\n",
    "                nn.Conv1d(EMBED_DIM // (2**i), EMBED_DIM // (2**i), 3, padding=1),\n",
    "                nn.Sequential(\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dense_dropout),\n",
    "                    nn.BatchNorm1d(EMBED_DIM // (2**i)),\n",
    "                ) if not (i == 0 or j == conv_depth-1) else nn.Identity(),\n",
    "             ) for j in range(conv_depth)])\n",
    "            for i in reversed(range(halvings))])\n",
    "        self.conv_dropout = conv_dropout\n",
    "\n",
    "        self.ffn = nn.Sequential(*[nn.Sequential(\n",
    "                nn.Linear(latent_dim * SEQ_LEN, latent_dim * SEQ_LEN),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dense_dropout),\n",
    "            ) for _ in range(latent_depth)])\n",
    "\n",
    "    def get_cast_dtype(self):\n",
    "        return torch.float32\n",
    "\n",
    "    \"\"\"\n",
    "        LND -> LND s.t. [n_ctx, batch_size, d_model]\n",
    "    \"\"\"\n",
    "    def forward(self, x, attn_mask = None):\n",
    "        seq_len, batch_size, _ = x.shape\n",
    "\n",
    "        x = x.permute(1, 0, 2) # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.pos_encoding[None, :, :]\n",
    "\n",
    "        x = x.permute(0, 2, 1) # [batch_size, d_model, n_ctx]\n",
    "        residues = [x]\n",
    "        for block in self.down:\n",
    "            x = block(x)\n",
    "            \n",
    "            x = x.permute(0, 2, 1) # [batch_size, n_ctx, d_model / 2 ^ n]\n",
    "            x = F.max_pool1d(x, 3, stride=2, padding=1)\n",
    "            x = x.permute(0, 2, 1) # [batch_size, d_model / 2 ^ (n-1), , n_ctx]\n",
    "            \n",
    "            residues += [x]\n",
    "        \n",
    "        x = x.permute(0, 2, 1).flatten(1, 2) # [batch_size, n_ctx* d_model]\n",
    "        x = self.ffn(x)\n",
    "        x = F.dropout(x, self.dropout, self.training)\n",
    "        \n",
    "        x = x.view(batch_size, seq_len, -1).permute(0, 2, 1) # [batch_size, d_model / 2^n, n_ctx]\n",
    "        x = x + residues[-1]\n",
    "        for i, block in enumerate(self.up):\n",
    "            \n",
    "            x = x.permute(0, 2, 1) # [batch_size, n_ctx, d_model / 2 ^ n]\n",
    "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "            x = x.permute(0, 2, 1) # [batch_size, d_model / 2 ^ (n-1), , n_ctx]\n",
    "            \n",
    "            x = block(x)\n",
    "            \n",
    "            x = x + residues[-2-i]\n",
    "        \n",
    "        x = x.permute(2, 0, 1) # [n_ctx, batch_size, d_model]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Transformer with BadNet\n",
    "\n",
    "og_model.transformer = UNet(2, 2, 0.5).to(DEVICES[0])\n",
    "\n",
    "# model = nn.DataParallel(og_model.to(DEVICES[0]), device_ids=DEVICES)\n",
    "model = og_model\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591753\n",
      "(tensor([49406,   320,   786,   593,   320,   736, 11122,   525,   320,  2442,\n",
      "          617,  2966,   525,   320, 11795,  1759,   269, 49407,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0], dtype=torch.int32), tensor([ 0.0338,  0.0446, -0.0328,  ..., -0.0190,  0.0113, -0.0351]))\n",
      "tensor(0.1606)\n"
     ]
    }
   ],
   "source": [
    "class BabySet(data.Dataset):\n",
    "    def __init__(self, token_file, feature_file):\n",
    "        self.features = torch.load(feature_file)\n",
    "        self.tokens = torch.load(token_file)\n",
    "        assert(self.features.shape[0] == self.tokens.shape[0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return  self.tokens[index].to(torch.int), self.features[index]\n",
    "\n",
    "babyset = BabySet('./data/tokens.pt', './data/features.pt')\n",
    "\n",
    "trainset, validset, testset = data.random_split(babyset, [0.8, 0.1, 0.1])\n",
    "\n",
    "BATCH_SIZE=512\n",
    "NUM_WORKERS=2\n",
    "\n",
    "trainloader = data.DataLoader(trainset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "validloader = data.DataLoader(validset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "testloader  = data.DataLoader(testset,  shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "print(len(trainloader), len(validloader), len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloader, optimizer, scheduler, criterion, metric, train, verbose=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    with torch.set_grad_enabled(train):\n",
    "        t = tqdm(dataloader)\n",
    "        losses = np.zeros(len(t))\n",
    "        accs = np.zeros(len(t))\n",
    "        for i, (X, y) in enumerate(t):\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "            X, y = X.to(DEVICES[0]), y.to(DEVICES[0])\n",
    "            _, pred, _ = model(None, X)\n",
    "            loss = criterion(pred, y)#, torch.ones(X.shape[0], device=DEVICES[0]))\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "            acc = metric(pred, y).mean()\n",
    "            if verbose:\n",
    "                t.set_description(f'Loss = {loss:.4f}, Accuracy = {acc * 100:02.2f}%')\n",
    "            losses[i] = loss.detach().cpu().item()\n",
    "            accs[i] = acc.detach().cpu().item()\n",
    "        print('Done with epoch task')\n",
    "        return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH 01 =====\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep-gen/deep-gen-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Loss = 0.0010, Accuracy = 46.77%:  39%|███████████████████████████████████████████▏                                                                  | 182/463 [03:01<04:39,  1.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m===== EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m epoch_train_losses, epoch_train_accs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_train_losses)\n\u001b[1;32m     19\u001b[0m train_accs\u001b[38;5;241m.\u001b[39mappend(epoch_train_accs)\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, criterion, metric, train, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m acc \u001b[38;5;241m=\u001b[39m metric(pred, y)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m---> 23\u001b[0m     t\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m losses[i] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m accs[i] \u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/deep-gen-venv/lib/python3.8/site-packages/torch/_tensor.py:965\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "metric = nn.CosineSimilarity()\n",
    "\n",
    "EPOCHS = 5\n",
    "scheduler = None\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=EPOCHS*len(trainloader))\n",
    "\n",
    "train_losses = []\n",
    "train_accs   = []\n",
    "valid_losses = []\n",
    "valid_accs   = []\n",
    "\n",
    "for epoch in range(EPOCHS):    \n",
    "    print(f'===== EPOCH {epoch+1:02} =====')\n",
    "    print('Training...')\n",
    "    epoch_train_losses, epoch_train_accs = run_epoch(model, trainloader, optimizer, scheduler, criterion, metric, train=True)\n",
    "    train_losses.append(epoch_train_losses)\n",
    "    train_accs.append(epoch_train_accs)\n",
    "    print(f'Epoch Train Loss = {epoch_train_losses.mean():.4f}, Epoch Train Accuracy = {epoch_train_accs.mean() * 100:02.2f}%')\n",
    "\n",
    "    np.savetxt('train_losses.csv', np.asarray(train_losses))\n",
    "    np.savetxt('train_accs.csv',   np.asarray(train_accs))\n",
    "\n",
    "    print('Validating...')\n",
    "    epoch_valid_losses, epoch_valid_accs = run_epoch(model, validloader, None, None, criterion, metric, train=False)\n",
    "    valid_losses.append(epoch_valid_losses.mean())\n",
    "    valid_accs.append(epoch_valid_accs.mean())\n",
    "    print(f'Epoch Validation Loss = {epoch_valid_losses.mean():.4f}, Epoch Validation Accuracy = {epoch_valid_accs.mean() * 100:02.2f}%')\n",
    "\n",
    "    np.savetxt('valid_losses.csv', np.asarray(valid_losses))\n",
    "    np.savetxt('valid_accs.csv',   np.asarray(valid_accs))\n",
    "\n",
    "print(f'===== TESTING =====')\n",
    "test_losses, test_accs = run_epoch(model, testloader, optimizer, criterion, metric, train=False)\n",
    "print(f'Test Loss = {test_losses.mean():.4f}, Test Accuracy = {test_accs.mean() * 100:02.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.module.cpu(), 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-gen-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
